# Ensemble Learning â€“ Bagging & Boosting Examples

## ğŸ“Œ Overview
This repository demonstrates the use of *Ensemble Learning* techniques â€” specifically *Bagging* and *Boosting* â€” to improve the performance of Decision Tree models.  
Both examples use the *Diabetes dataset* for prediction.

## ğŸ“– About Ensemble Learning
Ensemble methods combine predictions from multiple models to create a more robust and accurate final model.  
- *Bagging (Bootstrap Aggregating)* â€“ Reduces variance by training multiple models on different random samples and averaging predictions.
- *Boosting* â€“ Reduces bias by sequentially training models, giving more weight to previously misclassified data points.

## ğŸ›  Technologies Used
- Python 
- Pandas
- NumPy
- Scikit-Learn

## ğŸ“‚ Projects Included
### 1ï¸âƒ£ Decision Tree with Bagging
- Uses multiple Decision Tree classifiers trained on random subsets of data.
- Aggregates predictions for better stability and reduced overfitting.

### 2ï¸âƒ£ Decision Tree with Boosting
- Trains Decision Trees sequentially.
- Adjusts sample weights to focus on misclassified data for improved accuracy.
